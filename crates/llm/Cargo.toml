[package]
name = "llm"
version = "0.1.0"
edition = "2021"

[dependencies]
anyhow = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
tokio = { workspace = true }
# HTTP client for OpenAI responses endpoint
reqwest = { version = "0.11", features = ["json", "rustls-tls"] }

# We'll implement a stub for llama_cpp
# llama_cpp = { workspace = true, optional = true }

[features]
full = []  # We'll re-enable this later when we have actual implementation 